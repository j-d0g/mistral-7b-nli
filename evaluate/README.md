# Mistral-7B NLI Inference Guide

This guide explains how to run inference using the fine-tuned Mistral-7B NLI models, focusing on the optimized and unified inference script.

## Requirements

- NVIDIA GPU with sufficient VRAM (at least 16GB recommended for 7B models with QLoRA)
- Docker installed and configured for GPU usage
- The project's Docker image built (`docker build -t mistral-nli-ft .`)
- Trained model adapters (downloaded or from training output) located in the `models/` directory.

## Optimized Inference Workflow

The inference process (`run_inference.sh` wrapping `evaluate/sample_model.py`) is optimized for efficiency:

1.  **4-bit Quantization**: Uses `bitsandbytes` for loading the base model in 4-bit, reducing memory usage significantly. The LoRA adapters are then applied on top.
2.  **Batch Processing**: Processes the input data in batches (default size 32, configurable in `sample_model.py`) to maximize GPU throughput.
3.  **Sequence Length**: Uses a maximum sequence length relevant to the task (e.g., 512), balancing context needs with memory and speed.
4.  **Flash Attention 2**: Leverages Flash Attention if available for faster processing.
5.  **Checkpoint System (for `sample_model.py`)**: The underlying Python script includes logic to save progress periodically and resume if interrupted (though the wrapper `run_inference.sh` runs it as a single process).
6.  **Consistent Prompting**: Uses the same instruction prompt format (`[INST]...[/INST]`) as during fine-tuning for optimal model performance.
7.  **Flexible Data Handling**: The script automatically detects if the input CSV file has a `label` column.
    *   **With Labels**: Calculates and reports accuracy.
    *   **Without Labels**: Generates predictions only.

**Performance:** These optimizations resulted in significant speedups (e.g., >3x faster than initial estimates) and efficient VRAM usage (e.g., ~16GB on a 24GB card) during development.

## Running Inference with `run_inference.sh`

The recommended way to run inference is using the unified wrapper script from the project root:

```bash
# Run with default parameters (uses models/mistral-7b-nli-cot and data/sample/demo.csv)
./run_inference.sh

# Run with a specific model adapter directory and test dataset
./run_inference.sh --model models/Mistral_Thinking_Abl2/checkpoint-2000 --data data/original_data/test.csv

# Run inference using a specific checkpoint from a training run
./run_inference.sh --model models/Mistral_Thinking_Abl2/checkpoint-2000 --data data/finetune/dev_ft.jsonl

# Specify the GPU to use (default is GPU 0)
./run_inference.sh --model models/Mistral_Thinking_Abl2/checkpoint-2000 --data data/original_data/test.csv --gpu 1
```

### Script Parameters

-   `--model`, `-m`: Path to the fine-tuned model adapter directory or a specific checkpoint directory. (Default: `models/mistral-7b-nli-cot`)
-   `--data`, `-d`: Path to the input data file (CSV format, optionally with a `label` column). (Default: `data/sample/demo.csv`)
-   `--gpu`, `-g`: ID of the GPU to use for inference. (Default: `0`)
-   `--help`, `-h`: Display the help message and exit.

### Output Files

The script saves results in the `results/` directory with automatically generated, descriptive filenames:

-   **JSON Output**: `results/[model_name]-[dataset_name]-[timestamp].json`
    -   Contains detailed configuration, overall metrics (if labels exist), timing, and per-sample predictions including the generated `thought_process` and `predicted_label`.
-   **CSV Output**: `results/[model_name]-[dataset_name]-[timestamp].csv`
    -   A flattened version containing the original premise/hypothesis, the true label (if available), and the model's `predicted_label`.

Where:
-   `[model_name]` is derived from the path provided via `--model`.
-   `[dataset_name]` is derived from the path provided via `--data`.
-   `[timestamp]` is the execution time.

## Understanding the Output JSON

The primary output file (`.json`) contains:
-   `model`: Path/name of the model used.
-   `accuracy`: Overall accuracy (only if input data had labels).
-   `inference_time_seconds`: Total wall-clock time for the prediction loop.
-   `samples_per_second`: Throughput calculation.
-   `batch_size`: The batch size used for inference.
-   `max_length`: Max sequence length used.
-   `results`: A list where each item corresponds to an input sample and contains:
    -   Original data fields (e.g., `premise`, `hypothesis`).
    -   `true_label` (if available).
    -   `predicted_label`: The model's final prediction (0 or 1).
    -   `thought_process`: The step-by-step reasoning generated by the model.
    -   `full_output`: The complete text generated by the model before JSON parsing.
    -   `prediction_valid`: Boolean indicating if the generated output could be parsed correctly.

## Advanced Usage (Directly calling `sample_model.py`)

For finer control over parameters like batch size or max sequence length, you can bypass the wrapper script and call `evaluate/sample_model.py` directly within the Docker container:

```bash
# Example running sample_model.py directly
docker run --rm --gpus device=0 \
  -v $(pwd):/app \
  -v $(pwd)/hf_cache:/root/.cache/huggingface \
  -w /app \
  mistral-nli-ft \
  python evaluate/sample_model.py \
    --model_id "models/Mistral_Thinking_Abl2/checkpoint-2000" \
    --test_file "data/original_data/test.csv" \
    --output_file "results/custom_run_predictions.json" \
    --batch_size 64 \
    --max_length 512
```
*(Note: Adjust `model_id`, `test_file`, `output_file`, `batch_size`, `max_length`, and GPU device ID as needed)*

## Troubleshooting

1.  **CUDA Out of Memory (OOM) Errors**: If you encounter errors like `torch.cuda.OutOfMemoryError`:
    *   **Solution**: The most likely cause is the batch size. While the default is 32, try running `sample_model.py` directly (as shown above) with a smaller `--batch_size` (e.g., 16, 8, or even 4). You cannot currently change the batch size via `run_inference.sh`.
2.  **Slow Inference**:
    *   Ensure Docker is correctly configured to use the NVIDIA GPU (`nvidia-docker` or equivalent setup). Check `nvidia-smi` inside the container.
    *   Ensure Flash Attention 2 is installed and usable (should be handled by the Dockerfile).
3.  **`FileNotFoundError`**:
    *   Double-check the paths provided to `--model` and `--data`. Ensure the model adapter files (`adapter_config.json`, `adapter_model.bin`, etc.) exist within the specified model directory.
4.  **Invalid Prediction Output / Low Accuracy**:
    *   Ensure the model path points to the correct fine-tuned adapters.
    *   Verify the input data format matches what the model was trained on.
    *   Confirm the prompt structure used in `sample_model.py` aligns with the fine-tuning prompt.

## Key Evaluation Findings & Historical Context

This section provides context on model performance based on evaluations run during development, including addressing initial challenges.

**Performance after Fixing Extraction Logic:**

Initial evaluations were impacted by flawed logic for extracting the final prediction from model outputs containing Chain-of-Thought reasoning. After implementing a more robust extraction method (handling multiple JSONs, avoiding simple string checks), the performance comparison on the test set was:

| Model                | Fixed Accuracy | Notes                                      |
| -------------------- | -------------- | ------------------------------------------ |
| Base (Mistral-7B-v0.3) | ~53%           | Original model without fine-tuning         |
| Fine-tuned (Ablation 2) | ~91%           | Significant improvement (+38% vs Base)     |
| Checkpoint-1250      | ~82%           | Early checkpoint showing good progress   |

**Key Historical Challenges Addressed:**

1.  **Extraction Bias:** The original extraction logic incorrectly interpreted outputs containing "step 1:" as predicting label 1, artificially deflating the apparent performance of CoT-generating models.
2.  **Multiple JSONs:** Fine-tuned models sometimes generated multiple JSON objects; the extraction logic needed refinement to correctly parse the intended one.
3.  **Training Tokenizer Config:** An early training issue where `tokenizer.pad_token` was set to `tokenizer.eos_token` potentially hindered the model's ability to learn proper output termination. This was fixed in later training runs by adding a distinct `[PAD]` token.

This context highlights the effectiveness of the fine-tuning process and the importance of robust evaluation procedures. 