# Mistral-7B NLI Inference Guide

**Note:** The main project `README.md` contains the canonical, consolidated overview of the inference process. This file retains specific details, examples, and historical context related to the evaluation setup and findings for reference purposes.

This guide explains how to run inference using the fine-tuned Mistral-7B NLI models, focusing on the optimized and unified inference script.

## Optimized Inference Workflow

The inference process (`run_inference.sh` wrapping `evaluate/sample_model.py`) is optimized for efficiency:

1.  **4-bit Quantization**: Uses `bitsandbytes` for loading the base model in 4-bit, reducing memory usage significantly. The LoRA adapters are then applied on top.
2.  **Batch Processing**: Processes the input data in batches (default size 32, configurable in `sample_model.py`) to maximize GPU throughput.
3.  **Sequence Length**: Uses a maximum sequence length relevant to the task (e.g., 512), balancing context needs with memory and speed.
4.  **Flash Attention 2**: Leverages Flash Attention if available for faster processing.
5.  **Checkpoint System (for `sample_model.py`)**: The underlying Python script includes logic to save progress periodically and resume if interrupted (though the wrapper `run_inference.sh` runs it as a single process).
6.  **Consistent Prompting**: Uses the same instruction prompt format (`[INST]...[/INST]`) as during fine-tuning for optimal model performance.
7.  **Flexible Data Handling**: The script automatically detects if the input CSV file has a `label` column.
    *   **With Labels**: Calculates and reports accuracy.
    *   **Without Labels**: Generates predictions only.

**Performance:** These optimizations resulted in significant speedups (e.g., >3x faster than initial estimates) and efficient VRAM usage (e.g., ~16GB on a 24GB card) during development.

## Running Inference with `run_inference.sh`

The recommended way to run inference is using the unified wrapper script from the project root:

```bash
# Run with default parameters (uses models/mistral-7b-nli-cot and data/sample/demo.csv)
./run_inference.sh

# Run with a specific model adapter directory and test dataset
./run_inference.sh --model models/Mistral_Thinking_Abl2/checkpoint-2000 --data data/original_data/test.csv

# Run inference using a specific checkpoint from a training run
./run_inference.sh --model models/Mistral_Thinking_Abl2/checkpoint-2000 --data data/finetune/dev_ft.jsonl

# Specify the GPU to use (default is GPU 0)
./run_inference.sh --model models/Mistral_Thinking_Abl2/checkpoint-2000 --data data/original_data/test.csv --gpu 1
```

### Script Parameters

-   `--model`, `-m`: Path to the fine-tuned model adapter directory or a specific checkpoint directory. (Default: `models/mistral-7b-nli-cot`)
-   `--data`, `-d`: Path to the input data file (CSV format, optionally with a `label` column). (Default: `data/sample/demo.csv`)
-   `--gpu`, `-g`: ID of the GPU to use for inference. (Default: `0`)
-   `--help`, `-h`: Display the help message and exit.


## Understanding the Output JSON

The primary output file (`.json`) contains:
-   `model`: Path/name of the model used.
-   `accuracy`: Overall accuracy (only if input data had labels).
-   `inference_time_seconds`: Total wall-clock time for the prediction loop.
-   `samples_per_second`: Throughput calculation.
-   `batch_size`: The batch size used for inference.
-   `max_length`: Max sequence length used.
-   `results`: A list where each item corresponds to an input sample and contains:
    -   Original data fields (e.g., `premise`, `hypothesis`).
    -   `true_label` (if available).
    -   `predicted_label`: The model's final prediction (0 or 1).
    -   `thought_process`: The step-by-step reasoning generated by the model.
    -   `full_output`: The complete text generated by the model before JSON parsing.
    -   `prediction_valid`: Boolean indicating if the generated output could be parsed correctly.


