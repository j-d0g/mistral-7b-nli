{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset from HF\n",
    "Let's start by downloading the datasets from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully downloaded for jd0g/nlistral-7b-dataset into data/.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HF_TOKEN not found. Please set it.\")\n",
    "\n",
    "REPO_ID = \"jd0g/nlistral-7b-dataset\"\n",
    "LOCAL_DATA_DIR = Path(\"data\")\n",
    "\n",
    "def download_dataset():\n",
    "    os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    all_repo_files = list_repo_files(repo_id=REPO_ID, repo_type=\"dataset\", token=HF_TOKEN)\n",
    "    excluded_repo_paths = [\"download_dataset.py\", \"upload_dataset.py\", \"README.md\"] \n",
    "    files_to_download = [f for f in all_repo_files if f not in excluded_repo_paths]\n",
    "    \n",
    "    if not files_to_download:\n",
    "        if any(LOCAL_DATA_DIR.iterdir()): # Checks if directory is not empty\n",
    "             print(f\"No new files to download from {REPO_ID}. Local directory {LOCAL_DATA_DIR} not empty.\")\n",
    "             return\n",
    "\n",
    "    for file_path_in_repo in files_to_download:\n",
    "        try:\n",
    "            hf_hub_download(\n",
    "                repo_id=REPO_ID,\n",
    "                filename=file_path_in_repo,\n",
    "                repo_type=\"dataset\",\n",
    "                token=HF_TOKEN,\n",
    "                local_dir=LOCAL_DATA_DIR,\n",
    "                local_dir_use_symlinks=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {file_path_in_repo}: {e}\") \n",
    "            \n",
    "    print(f\"Dataset successfully downloaded for {REPO_ID} into {LOCAL_DATA_DIR}/.\")\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a Bombay looks uh i guess the closest thing to...</td>\n",
       "      <td>Is it because Halloween cat caricatures and Bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sailing north to find and claim islands for th...</td>\n",
       "      <td>Columbus traveled all over the world for the s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>his conviction was later overturned on appeal.</td>\n",
       "      <td>He was found guilty of Conspiracy to Rob but h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You tread on her foot, or pick up her handkerc...</td>\n",
       "      <td>Don't tread on her foot, or pick up her handke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  a Bombay looks uh i guess the closest thing to...   \n",
       "1  Sailing north to find and claim islands for th...   \n",
       "2     his conviction was later overturned on appeal.   \n",
       "3  In practice, and depending on the language, th...   \n",
       "4  You tread on her foot, or pick up her handkerc...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0  Is it because Halloween cat caricatures and Bo...      1  \n",
       "1  Columbus traveled all over the world for the s...      1  \n",
       "2  He was found guilty of Conspiracy to Rob but h...      1  \n",
       "3  In practice, and depending on the language, th...      1  \n",
       "4  Don't tread on her foot, or pick up her handke...      1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('data/original_data/sample.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart Model Download\n",
    "Single-Cell set-up for downloading a specific model ablation (best configuration), base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter 'nlistral-7b-qlora-ablation1-best' from Hub repo 'jd0g/nlistral-7b-qlora'...\n",
      "Base model specified in adapter config: mistralai/Mistral-7B-v0.3\n",
      "Loading base model 'mistralai/Mistral-7B-v0.3' with 4-bit quantization on GPU 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8a597e542b484d8cc4c78945cb7340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded.\n",
      "Loading tokenizer from Hub: 'jd0g/nlistral-7b-qlora' (subfolder: 'nlistral-7b-qlora-ablation1-best')...\n",
      "Tokenizer loaded.\n",
      "Resizing token embeddings of base model from 32768 to 32769.\n",
      "Applying PEFT adapter 'nlistral-7b-qlora-ablation1-best' from Hub to the base model...\n",
      "PEFT adapter applied. Model is ready.\n",
      "\n",
      "Model and tokenizer for 'jd0g/nlistral-7b-qlora/nlistral-7b-qlora-ablation1-best' loaded successfully from Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Configuration ---\n",
    "HUB_REPO_ID = \"jd0g/nlistral-7b-qlora\"\n",
    "ADAPTER_NAME_ON_HUB = \"nlistral-7b-qlora-ablation1-best\" # This is the subfolder name on the Hub\n",
    "GPU_ID = 0\n",
    "# ---\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Loading adapter '{ADAPTER_NAME_ON_HUB}' from Hub repo '{HUB_REPO_ID}'...\")\n",
    "\n",
    "# Download adapter_config.json from the subfolder on the Hub\n",
    "try:\n",
    "    adapter_config = PeftConfig.from_pretrained(HUB_REPO_ID, subfolder=ADAPTER_NAME_ON_HUB, token=HF_TOKEN)\n",
    "    BASE_MODEL_ID = adapter_config.base_model_name_or_path\n",
    "    print(f\"Base model specified in adapter config: {BASE_MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch PeftConfig. Error: {e}\")\n",
    "    print(f\"Falling back to default BASE_MODEL_ID: mistralai/Mistral-7B-v0.3\") # Fallback\n",
    "    BASE_MODEL_ID = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "\n",
    "#  Configure 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load the base model with quantization\n",
    "print(f\"Loading base model '{BASE_MODEL_ID}' with 4-bit quantization on GPU {GPU_ID}...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": GPU_ID},\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# Load the tokenizer directly from the adapter's location on the Hub\n",
    "print(f\"Loading tokenizer from Hub: '{HUB_REPO_ID}' (subfolder: '{ADAPTER_NAME_ON_HUB}')...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HUB_REPO_ID,\n",
    "    subfolder=ADAPTER_NAME_ON_HUB, # Specify the subfolder\n",
    "    use_fast=True,\n",
    "    padding_side=\"left\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Resize token embeddings if necessary\n",
    "if len(tokenizer) != base_model.config.vocab_size:\n",
    "    print(f\"Resizing token embeddings of base model from {base_model.config.vocab_size} to {len(tokenizer)}.\")\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Apply the PEFT adapter to the base model, loading adapter weights from the Hub\n",
    "print(f\"Applying PEFT adapter '{ADAPTER_NAME_ON_HUB}' from Hub to the base model...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    HUB_REPO_ID, # Main repository ID\n",
    "    subfolder=ADAPTER_NAME_ON_HUB, # Specify the subfolder for the adapter\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "model.eval() # Set the model to evaluation mode\n",
    "print(\"PEFT adapter applied. Model is ready.\")\n",
    "\n",
    "# Set pad_token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer pad_token is None. Setting it to eos_token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 'model' and 'tokenizer' are now ready for use.\n",
    "ADAPTER_PATH = f\"{HUB_REPO_ID}/{ADAPTER_NAME_ON_HUB}\"\n",
    "\n",
    "print(f\"\\nModel and tokenizer for '{ADAPTER_PATH}' loaded successfully from Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL) Download ALL Fine-Tuned QLoRA Adaptors Trained, Load Mistral Base Model & Tokenizer\n",
    "If you want to download all the models locally and compare different ablations, you can run the below two cells in this section. Skip if you already ran the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: nlistral-7b-qlora-ablation1-best/adapter_config.json\n",
      "Downloading: nlistral-7b-qlora-ablation1-best/adapter_model.safetensors\n",
      "Downloading: nlistral-7b-qlora-ablation1-best/special_tokens_map.json\n",
      "Downloading: nlistral-7b-qlora-ablation1-best/tokenizer.model\n",
      "Downloading: nlistral-7b-qlora-ablation1-best/tokenizer.json\n",
      "Downloading: nlistral-7b-qlora-ablation1-best/tokenizer_config.json\n",
      "✅ Model downloaded to: models/nlistral-ablation1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "REPO_ID = \"jd0g/nlistral-7b-qlora\"\n",
    "LOCAL_MODEL_DIR = Path(\"models\")\n",
    "model_path = \"nlistral-7b-qlora-ablation1-best\"\n",
    "local_dir = \"nlistral-ablation1\"\n",
    "\n",
    "model_dir = LOCAL_MODEL_DIR / local_dir\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for file_name in [\"adapter_config.json\", \"adapter_model.safetensors\", \"special_tokens_map.json\", \"tokenizer.model\", \"tokenizer.json\", \"tokenizer_config.json\"]:\n",
    "    file_path = f\"{model_path}/{file_name}\"\n",
    "    print(f\"Downloading: {file_path}\")\n",
    "    temp_file = hf_hub_download(repo_id=REPO_ID, filename=file_path, token=HF_TOKEN, local_dir=LOCAL_MODEL_DIR)\n",
    "    dest_path = model_dir / file_name\n",
    "    if os.path.dirname(temp_file) != str(model_dir): \n",
    "        import shutil; shutil.copy2(temp_file, dest_path); os.remove(temp_file)\n",
    "\n",
    "print(f\"✅ Model downloaded to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf7ed34109f44bb9829c0d27d7c304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Configuration ---\n",
    "ADAPTER_PATH = \"models/nlistral-ablation1\"  # Path to your adapter checkpoint directory\n",
    "GPU_ID = 0                                 # GPU to load the model on\n",
    "# ---\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "adapter_config_file = os.path.join(ADAPTER_PATH, \"adapter_config.json\")\n",
    "with open(adapter_config_file, 'r') as f:\n",
    "    adapter_config = json.load(f)\n",
    "BASE_MODEL_ID = adapter_config.get(\"base_model_name_or_path\", \"mistralai/Mistral-7B-v0.3\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\" quenched\": GPU_ID},\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    ADAPTER_PATH,\n",
    "    use_fast=True,\n",
    "    padding_side=\"left\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "if len(tokenizer) != base_model.config.vocab_size:\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 'model' and 'tokenizer' are now ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a Single Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"thought_process\": \"step 1: the premise states that the cat is on the rug. a rug is a type of mat, often used for floor coverings. step 2: the hypothesis suggests that a feline is on the mat. since a mat can refer to a rug, and the cat is on the rug, it can be inferred that a feline is on the mat. step 3: based on the logical reasoning and lack of contradictory facts, the hypothesis can be inferred from the premise.\", \"predicted_label\": 1} '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.prompts import FINETUNE_PROMPT\n",
    "FINETUNE_PROMPT\n",
    "\n",
    "premise = \"The cat is on the rug.\"\n",
    "hypothesis = \"A feline is on the mat.\"\n",
    "\n",
    "formatted_prompt = FINETUNE_PROMPT.format(\n",
    "    premise=premise,\n",
    "    hypothesis=hypothesis\n",
    ")\n",
    "\n",
    "input_text = f\"[INST] {formatted_prompt} [/INST]\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output_sequences = model.generate(**inputs, max_new_tokens=256, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print only the generated part\n",
    "generated_text = tokenizer.decode(output_sequences[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process outputs from an input CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 rows from data/original_data/sample.csv to create nlistral-7b-qlora-ablation1-best_sample_predictions.csv using FINETUNE_PROMPT.\n",
      "  Processed 1/5 items.\n",
      "  Processed 2/5 items.\n",
      "  Processed 3/5 items.\n",
      "  Processed 4/5 items.\n",
      "  Processed 5/5 items.\n",
      "\n",
      "Finished processing. Output saved to: nlistral-7b-qlora-ablation1-best_sample_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a Bombay looks uh i guess the closest thing to...</td>\n",
       "      <td>Is it because Halloween cat caricatures and Bo...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sailing north to find and claim islands for th...</td>\n",
       "      <td>Columbus traveled all over the world for the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>his conviction was later overturned on appeal.</td>\n",
       "      <td>He was found guilty of Conspiracy to Rob but h...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You tread on her foot, or pick up her handkerc...</td>\n",
       "      <td>Don't tread on her foot, or pick up her handke...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  a Bombay looks uh i guess the closest thing to...   \n",
       "1  Sailing north to find and claim islands for th...   \n",
       "2     his conviction was later overturned on appeal.   \n",
       "3  In practice, and depending on the language, th...   \n",
       "4  You tread on her foot, or pick up her handkerc...   \n",
       "\n",
       "                                          hypothesis  label  \\\n",
       "0  Is it because Halloween cat caricatures and Bo...      1   \n",
       "1  Columbus traveled all over the world for the s...      1   \n",
       "2  He was found guilty of Conspiracy to Rob but h...      1   \n",
       "3  In practice, and depending on the language, th...      1   \n",
       "4  Don't tread on her foot, or pick up her handke...      1   \n",
       "\n",
       "                                              output  \n",
       "0  {\"thought_process\": \"step 1: the premise descr...  \n",
       "1  {\"thought_process\": \"step 1: the premise descr...  \n",
       "2  {\"thought_process\": \"step 1: in the premise, i...  \n",
       "3  {\"thought_process\": \"step 1: the premise state...  \n",
       "4  {\"thought_process\": \"step 1: in the premise, t...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "CSV_FILE_PATH = 'data/original_data/sample.csv'\n",
    "MAX_NEW_TOKENS_PER_ITEM = 256 \n",
    "\n",
    "input_model_name = os.path.splitext(os.path.basename(ADAPTER_PATH))[0]\n",
    "input_data_name = os.path.splitext(os.path.basename(CSV_FILE_PATH))[0]\n",
    "OUTPUT_CSV_FILENAME = f\"{input_model_name}_{input_data_name}_predictions.csv\"\n",
    "OUTPUT_COLUMN_NAME = 'output' # As per your script\n",
    "\n",
    "# --- Script ---\n",
    "df = pd.read_csv(CSV_FILE_PATH) # Processing the whole CSV as per updated request\n",
    "df = pd.read_csv(CSV_FILE_PATH)[:5] # If you want to test with first 10 rows\n",
    "\n",
    "model_device = next(model.parameters()).device\n",
    "results_list = []\n",
    "\n",
    "print(f\"Processing {len(df)} rows from {CSV_FILE_PATH} to create {OUTPUT_CSV_FILENAME} using FINETUNE_PROMPT.\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    premise = str(row['premise'])\n",
    "    hypothesis = str(row['hypothesis'])\n",
    "\n",
    "    # Format the prompt using FINETUNE_PROMPT\n",
    "    formatted_core_prompt = FINETUNE_PROMPT.format(\n",
    "        premise=premise,\n",
    "        hypothesis=hypothesis\n",
    "    )\n",
    "    # Add Mistral instruction tags\n",
    "    input_text = f\"[INST] {formatted_core_prompt} [/INST]\"\n",
    "\n",
    "    max_model_length = getattr(model.config, 'max_position_embeddings', 512) \n",
    "    max_input_length = max_model_length - MAX_NEW_TOKENS_PER_ITEM\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=max_input_length \n",
    "    ).to(model_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS_PER_ITEM,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False # For deterministic output (recommended for structured tasks)\n",
    "        )\n",
    "    \n",
    "    # Decode only the newly generated text\n",
    "    generated_text = tokenizer.decode(output_sequences[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    results_list.append(generated_text)\n",
    "    print(f\"  Processed {index + 1}/{len(df)} items.\")\n",
    "    \n",
    "    del inputs, output_sequences\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "df[OUTPUT_COLUMN_NAME] = results_list\n",
    "df.to_csv(OUTPUT_CSV_FILENAME, index=False)\n",
    "\n",
    "print(f\"\\nFinished processing. Output saved to: {OUTPUT_CSV_FILENAME}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 rows from data/original_data/sample.csv in batches of 8 to create nlistral-ablation1_sample_predictions_batched.csv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db22606d5ed04eb4b70a3bd77b9d1df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Output saved to: nlistral-ablation1_sample_predictions_batched.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a Bombay looks uh i guess the closest thing to...</td>\n",
       "      <td>Is it because Halloween cat caricatures and Bo...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sailing north to find and claim islands for th...</td>\n",
       "      <td>Columbus traveled all over the world for the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>his conviction was later overturned on appeal.</td>\n",
       "      <td>He was found guilty of Conspiracy to Rob but h...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You tread on her foot, or pick up her handkerc...</td>\n",
       "      <td>Don't tread on her foot, or pick up her handke...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  a Bombay looks uh i guess the closest thing to...   \n",
       "1  Sailing north to find and claim islands for th...   \n",
       "2     his conviction was later overturned on appeal.   \n",
       "3  In practice, and depending on the language, th...   \n",
       "4  You tread on her foot, or pick up her handkerc...   \n",
       "\n",
       "                                          hypothesis  label  \\\n",
       "0  Is it because Halloween cat caricatures and Bo...      1   \n",
       "1  Columbus traveled all over the world for the s...      1   \n",
       "2  He was found guilty of Conspiracy to Rob but h...      1   \n",
       "3  In practice, and depending on the language, th...      1   \n",
       "4  Don't tread on her foot, or pick up her handke...      1   \n",
       "\n",
       "                                              output  \n",
       "0  {\"thought_process\": \"step 1: the premise descr...  \n",
       "1  {\"thought_process\": \"step 1: the premise descr...  \n",
       "2  {\"thought_process\": \"step 1: in the premise, i...  \n",
       "3  {\"thought_process\": \"step 1: the premise state...  \n",
       "4  {\"thought_process\": \"step 1: in the premise, t...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm # For a progress bar\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_PATH = 'data/original_data/sample.csv'\n",
    "MAX_NEW_TOKENS_PER_ITEM = 256\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Assumes ADAPTER_PATH is defined in the notebook scope from a previous cell\n",
    "input_model_name = os.path.splitext(os.path.basename(ADAPTER_PATH))[0]\n",
    "input_data_name = os.path.splitext(os.path.basename(CSV_FILE_PATH))[0]\n",
    "OUTPUT_CSV_FILENAME = f\"{input_model_name}_{input_data_name}_predictions_batched.csv\"\n",
    "OUTPUT_COLUMN_NAME = 'output'\n",
    "\n",
    "# --- Script ---\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "model_device = next(model.parameters()).device # Assumes model is loaded\n",
    "all_results = []\n",
    "\n",
    "print(f\"Processing {len(df)} rows from {CSV_FILE_PATH} in batches of {BATCH_SIZE} to create {OUTPUT_CSV_FILENAME}.\")\n",
    "print(f\"Using FINETUNE_PROMPT (first 100 chars): {FINETUNE_PROMPT[:100]}...\")\n",
    "max_model_length = getattr(model.config, 'max_position_embeddings', 512)\n",
    "max_input_length = max_model_length - MAX_NEW_TOKENS_PER_ITEM\n",
    "\n",
    "# Process data in batches\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "    batch_df = df.iloc[i:i+BATCH_SIZE]\n",
    "    batch_prompts = []\n",
    "\n",
    "    for index, row in batch_df.iterrows():\n",
    "        premise = str(row['premise'])\n",
    "        hypothesis = str(row['hypothesis'])\n",
    "        # Assumes FINETUNE_PROMPT is defined and loaded\n",
    "        formatted_core_prompt = FINETUNE_PROMPT.format(premise=premise, hypothesis=hypothesis)\n",
    "        input_text = f\"[INST] {formatted_core_prompt} [/INST]\"\n",
    "        batch_prompts.append(input_text)\n",
    "    \n",
    "    # Batch tokenization (assumes tokenizer is loaded)\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True, \n",
    "        max_length=max_input_length \n",
    "    ).to(model_device)\n",
    "\n",
    "    # Batch generation\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS_PER_ITEM,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    batch_generated_texts = []\n",
    "    prompt_len_in_batch = inputs[\"input_ids\"].shape[1]\n",
    "    for j in range(output_sequences.shape[0]):\n",
    "        generated_part = tokenizer.decode(output_sequences[j][prompt_len_in_batch:], skip_special_tokens=True)\n",
    "        batch_generated_texts.append(generated_part)\n",
    "        \n",
    "    all_results.extend(batch_generated_texts)\n",
    "    \n",
    "    del inputs, output_sequences, batch_prompts\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "df[OUTPUT_COLUMN_NAME] = all_results\n",
    "df.to_csv(OUTPUT_CSV_FILENAME, index=False)\n",
    "\n",
    "print(f\"\\nFinished processing. Output saved to: {OUTPUT_CSV_FILENAME}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing with JSON Extraction for predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 rows from data/original_data/sample.csv, BATCH_SIZE=16.\n",
      "Output (full): nlistral-ablation1_sample_full_outputs_batched.csv\n",
      "Output (preds): nlistral-ablation1_sample_final_predictions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76273e62e49742f4beefb974ad53497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full outputs saved to: nlistral-ablation1_sample_full_outputs_batched.csv\n",
      "Final predictions saved to: nlistral-ablation1_sample_final_predictions.csv\n",
      "\n",
      "Done. Displaying head of final predictions:\n",
      "   prediction\n",
      "0           1\n",
      "1           1\n",
      "2           1\n",
      "3           1\n",
      "4           1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- ASSUMED FROM PREVIOUS CELLS: ADAPTER_PATH, FINETUNE_PROMPT, model, tokenizer ---\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_PATH = 'data/original_data/sample.csv'\n",
    "MAX_NEW_TOKENS_PER_ITEM = 256\n",
    "BATCH_SIZE = 8 \n",
    "# ---\n",
    "\n",
    "input_model_name = os.path.splitext(os.path.basename(ADAPTER_PATH))[0]\n",
    "input_data_name = os.path.splitext(os.path.basename(CSV_FILE_PATH))[0]\n",
    "\n",
    "FULL_OUTPUT_CSV_FILENAME = f\"{input_model_name}_{input_data_name}_full_outputs_batched.csv\"\n",
    "FINAL_PREDICTIONS_CSV_FILENAME = f\"{input_model_name}_{input_data_name}_final_predictions.csv\"\n",
    "OUTPUT_COLUMN_NAME = 'generated_full_output'\n",
    "PREDICTION_COLUMN_NAME = 'prediction'\n",
    "\n",
    "def extract_simple_prediction(output_text):\n",
    "    output_text = str(output_text).strip()\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', output_text, re.DOTALL)\n",
    "        if match:\n",
    "            response_dict = json.loads(match.group(0))\n",
    "            label_value = response_dict.get('predicted_label', response_dict.get('label'))\n",
    "            if label_value is not None: return int(label_value)\n",
    "    except (json.JSONDecodeError, TypeError, ValueError): pass\n",
    "\n",
    "    for pattern in [r'predicted_label\\s*:\\s*([01])', \n",
    "                    r'final\\s+label\\s*:\\s*([01])', \n",
    "                    r'final\\s+prediction\\s*:\\s*([01])']:\n",
    "        match = re.search(pattern, output_text.lower())\n",
    "        if match: return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "model_device = next(model.parameters()).device\n",
    "all_full_outputs = []\n",
    "\n",
    "max_model_length = getattr(model.config, 'max_position_embeddings', 512)\n",
    "max_input_length = max_model_length - MAX_NEW_TOKENS_PER_ITEM\n",
    "\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Batch Processing\"):\n",
    "    batch_df = df.iloc[i:i+BATCH_SIZE]\n",
    "    batch_prompts = [\n",
    "        f\"[INST] {FINETUNE_PROMPT.format(premise=str(row['premise']), hypothesis=str(row['hypothesis']))} [/INST]\"\n",
    "        for _, row in batch_df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        batch_prompts, return_tensors=\"pt\", padding=True,\n",
    "        truncation=True, max_length=max_input_length \n",
    "    ).to(model_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            **inputs, max_new_tokens=MAX_NEW_TOKENS_PER_ITEM,\n",
    "            pad_token_id=tokenizer.eos_token_id, do_sample=False\n",
    "        )\n",
    "    \n",
    "    prompt_len_in_batch = inputs[\"input_ids\"].shape[1]\n",
    "    all_full_outputs.extend([\n",
    "        tokenizer.decode(output_sequences[j][prompt_len_in_batch:], skip_special_tokens=True)\n",
    "        for j in range(output_sequences.shape[0])\n",
    "    ])\n",
    "    \n",
    "    del inputs, output_sequences, batch_prompts\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "df[OUTPUT_COLUMN_NAME] = all_full_outputs\n",
    "df.to_csv(FULL_OUTPUT_CSV_FILENAME, index=False)\n",
    "print(f\"\\nFull outputs saved to: {FULL_OUTPUT_CSV_FILENAME}\")\n",
    "\n",
    "extracted_predictions = df[OUTPUT_COLUMN_NAME].apply(extract_simple_prediction)\n",
    "predictions_df = pd.DataFrame({PREDICTION_COLUMN_NAME: extracted_predictions})\n",
    "predictions_df.to_csv(FINAL_PREDICTIONS_CSV_FILENAME, index=False)\n",
    "print(f\"Final predictions saved to: {FINAL_PREDICTIONS_CSV_FILENAME}\")\n",
    "\n",
    "print(\"\\nDone. Displaying head of final predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    prediction\n",
       "0            1\n",
       "1            1\n",
       "2            1\n",
       "3            1\n",
       "4            1\n",
       "..         ...\n",
       "95           0\n",
       "96           0\n",
       "97           0\n",
       "98           1\n",
       "99           1\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(FINAL_PREDICTIONS_CSV_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>generated_full_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a Bombay looks uh i guess the closest thing to...</td>\n",
       "      <td>Is it because Halloween cat caricatures and Bo...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sailing north to find and claim islands for th...</td>\n",
       "      <td>Columbus traveled all over the world for the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>his conviction was later overturned on appeal.</td>\n",
       "      <td>He was found guilty of Conspiracy to Rob but h...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>In practice, and depending on the language, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You tread on her foot, or pick up her handkerc...</td>\n",
       "      <td>Don't tread on her foot, or pick up her handke...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>It is referred to as the Warwickshire Avon Avo...</td>\n",
       "      <td>It is also called the Warwickshire Avon or Sha...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>i still haven't checked out to see if they do ...</td>\n",
       "      <td>They don't make the 18.</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>From Adelaide, South Australia, Hele played cl...</td>\n",
       "      <td>From Adelaide, South Australia, Hele played cl...</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Poirot unlocked the door and, upon seeing her ...</td>\n",
       "      <td>Poirot recognized her distress the moment he g...</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"thought_process\": \"step 1: the premise state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>current on all you know several social issues ...</td>\n",
       "      <td>This man has a lot of time on his hands and no...</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"thought_process\": \"step 1: in the premise, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise  \\\n",
       "0   a Bombay looks uh i guess the closest thing to...   \n",
       "1   Sailing north to find and claim islands for th...   \n",
       "2      his conviction was later overturned on appeal.   \n",
       "3   In practice, and depending on the language, th...   \n",
       "4   You tread on her foot, or pick up her handkerc...   \n",
       "..                                                ...   \n",
       "95  It is referred to as the Warwickshire Avon Avo...   \n",
       "96  i still haven't checked out to see if they do ...   \n",
       "97  From Adelaide, South Australia, Hele played cl...   \n",
       "98  Poirot unlocked the door and, upon seeing her ...   \n",
       "99  current on all you know several social issues ...   \n",
       "\n",
       "                                           hypothesis  label  \\\n",
       "0   Is it because Halloween cat caricatures and Bo...      1   \n",
       "1   Columbus traveled all over the world for the s...      1   \n",
       "2   He was found guilty of Conspiracy to Rob but h...      1   \n",
       "3   In practice, and depending on the language, th...      1   \n",
       "4   Don't tread on her foot, or pick up her handke...      1   \n",
       "..                                                ...    ...   \n",
       "95  It is also called the Warwickshire Avon or Sha...      1   \n",
       "96                            They don't make the 18.      0   \n",
       "97  From Adelaide, South Australia, Hele played cl...      0   \n",
       "98  Poirot recognized her distress the moment he g...      1   \n",
       "99  This man has a lot of time on his hands and no...      0   \n",
       "\n",
       "                                generated_full_output  \n",
       "0   {\"thought_process\": \"step 1: the premise descr...  \n",
       "1   {\"thought_process\": \"step 1: the premise descr...  \n",
       "2   {\"thought_process\": \"step 1: in the premise, i...  \n",
       "3   {\"thought_process\": \"step 1: the premise state...  \n",
       "4   {\"thought_process\": \"step 1: in the premise, t...  \n",
       "..                                                ...  \n",
       "95  {\"thought_process\": \"step 1: the premise state...  \n",
       "96  {\"thought_process\": \"step 1: in the premise, t...  \n",
       "97  {\"thought_process\": \"step 1: the premise state...  \n",
       "98  {\"thought_process\": \"step 1: the premise state...  \n",
       "99  {\"thought_process\": \"step 1: in the premise, i...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(FULL_OUTPUT_CSV_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
