# NLIstral-QLoRA-7B: Interpretable NLI with Augmented Chain-of-Thought Fine-Tuning

This document provides instructions for evaluating NLI models on test datasets, whether you've trained your own models or downloaded pre-trained ones.

## Table of Contents
1. [Introduction and Motivation](#introduction-and-motivation)
   - 1.1. [Abstract](#synthetic-data-augmentation)
   - 1.2. [Motivation & Background](#synthetic-data-augmentation)
   - 1.2. [Overview of Architecture](#synthetic-data-augmentation)
3. [Methodology](#methodology)
   - 3.1. [Synthetic Data Augmentation](#synthetic-data-augmentation)
   - 3.2. [Model Architecture](#model-architecture)
   - 3.3. [Training Approach](#training-approach)
4. [Experiment I - Synthetic Data Augmentaton](#experiments)
   - 4.1. [Initial Experiments: LR-ning From Failure](#experimental-setup)
   - 4.2. [Less-Is-All-You-Need: Improved Thought Generation](#experimental-setup)
   - 4.2. [LLM-As-A-Judge: Iterative Self-Critique & Improvement](#experimental-setup)
   - 4.4. [Learn-From-Your-Mistakes: Self-Reflection & Correction](#experimental-setup)
   - 4.5. [Deciding Datasets:](#experimental-setup)
5. [Experiment II - Fine-Tuning with QLoRA](#experiments)
   - 5.1. [Dockerised SSH Training Set-Up](#experimental-setup)
   - 5.2. [Mistral-7B: 4-bit Quantized Model](#experimental-setup)
   - 5.3. [Parameter-Efficient Fine-Tuning with QLoRA](#experimental-setup)
   - 5.4. [Ablation Studies & Hyper-Parameter Tuning](#ablation-studies)
6. [Results](#results)
   - 6.1. [Baseline Performance](#quantitative-analysis)
   - 6.2. [Fine-Tuned Performance](#qualitative-analysis)
   - 6.3. [Benchmarks](#qualitative-analysis)
   - 6.4. [Thought Quality](#qualitative-analysis)
7. [Discussion](#discussion)
   - 7.1. [Model Bias](#qualitative-analysis)
   - 7.2. [Labeller Bias](#qualitative-analysis)
   - 7.3. [Trade-Offs and Assumptions](#qualitative-analysis)
8. [Limitations and Future Work](#limitations-and-future-work)
   - 8.1. [Self-Consistency](#self-consistency)
   - 8.2. [Tree-of-Thought](#tree-of-thought)
   - 8.3. [GRPO Optimization](#RL-(grpo))
   - 8.4. [Refining Scoring](#refining-scoring)
   - 8.5. [Full Fine-Tuning](#full-fine-tuning)
9. [Conclusion](#conclusion)
10. [References](#references)